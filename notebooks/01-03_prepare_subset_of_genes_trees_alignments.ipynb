{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare subset of gene trees and alignments for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Make a list of all genes in the Gold-EggNOG overlap, and find GFF files for the corresponding genome assemblies\n",
    "    - We need the nucleotide sequences for later analyses of selection on HGT\n",
    "2. Make a list of all taxa and genes for which nucleotide sequences are available\n",
    "3. Prepare a list of gene families (NOGs) that contain at least 4 of these taxa\n",
    "3. Prepare sets of gene trees for HGT inference (at least 4 taxa each)\n",
    "4. Select a subset of gene families (broadest possible, such that at least 95% of the selected taxa are present in each family) for estimating the genome tree\n",
    "4. Extract these subsets of gene trees and alignments from the EggNOG database\n",
    "5. Prune all gene trees to contain only the selected taxa and genes that have nucleotide sequences available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress SyntaxWarnings in ete3 import using the warnings module\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "import gffutils\n",
    "import tqdm\n",
    "import ete3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "gold_e6_overlap_dir = os.path.join(data_dir, 'gold_e6_overlap')\n",
    "e6_dir = os.path.join(data_dir, 'eggnog6')\n",
    "\n",
    "filtered_dir = os.path.join(data_dir, 'filtered')\n",
    "if not os.path.exists(filtered_dir):\n",
    "    os.makedirs(filtered_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique genes: 750755\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "filtered_taxa_filepath = os.path.join(gold_e6_overlap_dir, 'e6_gold.filtered_taxa.txt')\n",
    "nog2taxa_filepath = os.path.join(e6_dir, 'e6.og2seqs_and_species_bacteria.tsv')\n",
    "filtered_genes_list_filepath = os.path.join(filtered_dir, 'genes.taxa_filtered.txt')\n",
    "\n",
    "# Read in the gold-e6 overlap taxa list\n",
    "filtered_taxa_df = pd.read_csv(filtered_taxa_filepath, sep='\\t', names=['taxon_ID'], dtype={'taxon_ID': str})\n",
    "filtered_taxa_set = set(filtered_taxa_df['taxon_ID'])\n",
    "\n",
    "# Read the bacterial OGs to taxa mapping file in chunks\n",
    "chunk_size = 10000\n",
    "nog2taxa_chunks = pd.read_csv(nog2taxa_filepath, sep='\\t', header=None, names=['level', 'nog', 'taxa_count', 'members_count', 'taxa', 'members'], chunksize=chunk_size)\n",
    "\n",
    "unique_filtered_genes = set()\n",
    "\n",
    "for chunk in nog2taxa_chunks:\n",
    "    # Convert taxa and members columns into list of strings\n",
    "    chunk['taxa'] = chunk['taxa'].str.split(',')\n",
    "    chunk['members'] = chunk['members'].str.split(',')\n",
    "    chunk['taxa_count'] = chunk['taxa_count'].astype(int)\n",
    "\n",
    "    # Filter taxa to keep only those in the sampled taxa list\n",
    "    chunk['filtered_taxa'] = chunk['taxa'].apply(lambda taxa_list: [taxon for taxon in taxa_list if taxon in filtered_taxa_set])\n",
    "    chunk['filtered_taxa_count'] = chunk['filtered_taxa'].apply(len)\n",
    "\n",
    "    # Drop NOGs that have less than 4 taxa\n",
    "    chunk = chunk[chunk['filtered_taxa_count'] >= 4]\n",
    "\n",
    "    # Filter members to keep only those that belong to the filtered taxa\n",
    "    chunk['filtered_members'] = chunk['members'].apply(lambda members_list: [member for member in members_list if member.split('.')[0] in filtered_taxa_set])\n",
    "\n",
    "    # Combine all filtered members into a single list and remove duplicates\n",
    "    for members_list in chunk['filtered_members']:\n",
    "        unique_filtered_genes.update(members_list)\n",
    "\n",
    "# Write the unique filtered genes to a file\n",
    "with open(filtered_genes_list_filepath, 'w') as file:\n",
    "    for gene in unique_filtered_genes:\n",
    "        file.write(f'{gene}\\n')\n",
    "\n",
    "# Print summary\n",
    "print(f\"Number of unique genes: {len(unique_filtered_genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find GFF files that contain the gene IDs for the Gold-EggNOG overlap\n",
    "\n",
    "```bash\n",
    "nohup python code/download_eggnog_ncbi_genome_assemblies.py -i data/filtered/genes.taxa_filtered.txt -o data/genome_data/ -c ~/ncbi_credentials.txt  > data/nohup_gff3_dload.log & disown\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxa with available GFF files: 159\n",
      "Example taxa with available GFF files: [104087, 105219, 106654, 109790, 1221500]\n"
     ]
    }
   ],
   "source": [
    "# make a list of all taxa for which we have available gene features\n",
    "found_locus_tags_count_filepath = os.path.join(\n",
    "    data_dir, 'genome_data/found_locus_tags_count.tsv')\n",
    "found_locus_tags_count_df = pd.read_csv(found_locus_tags_count_filepath, sep='\\t', header=0)\n",
    "filtered_taxa_w_available_gff = found_locus_tags_count_df['taxon_id']\n",
    "# write it to a file\n",
    "filtered_taxa_w_available_gff_filepath = os.path.join(\n",
    "    filtered_dir, 'taxa.overlap_filtered.gff_filtered.txt')\n",
    "filtered_taxa_w_available_gff.to_csv(filtered_taxa_w_available_gff_filepath, index=False, header=False)\n",
    "# display some\n",
    "print(f\"Number of taxa with available GFF files: {len(filtered_taxa_w_available_gff)}\")\n",
    "print(f\"Example taxa with available GFF files: {filtered_taxa_w_available_gff.head().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [00:00<00:00, 220242.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted gene information for 159 genes.\n"
     ]
    }
   ],
   "source": [
    "# read in the list of locus tags found in the gff files\n",
    "found_locus_tags_filepath = os.path.join(\n",
    "    data_dir, 'genome_data/found_locus_tags.txt') # this file is created by the script 'code/download_eggnog_ncbi_genome_assemblies.py'\n",
    "with open(found_locus_tags_filepath, 'r') as file:\n",
    "    found_locus_tags = file.read().splitlines()\n",
    "\n",
    "# make a list of all gene IDs searched for\n",
    "gene_ids_filepath = os.path.join(filtered_dir, 'genes.taxa_filtered.txt')\n",
    "gene_ids_df = pd.read_csv(gene_ids_filepath, header=None, names=['gene_id'])\n",
    "gene_ids_df['taxon_id'] = gene_ids_df['gene_id'].str.split('.').str[0]\n",
    "gene_ids_df['locus_tag'] = gene_ids_df['gene_id'].str.split('.').str[1]\n",
    "\n",
    "# filter out locus tags that were not found in the gff files\n",
    "found_gene_ids_df = gene_ids_df[gene_ids_df['locus_tag'].isin(found_locus_tags)]\n",
    "\n",
    "# we now go through the gff files and extract the information for the genes we are interested in\n",
    "gff_dir = os.path.join(data_dir, 'genome_data')\n",
    "gff_files = [f for f in os.listdir(gff_dir) if f.endswith('.gff')]\n",
    "gff_files = [os.path.join(gff_dir, f) for f in gff_files]\n",
    "# function to extract gene information from a gff file and make a json file of locus tag features\n",
    "def extract_gene_info_from_gff(gff_file, taxon_gene_ids_df):\n",
    "    \"\"\"\n",
    "    Extract gene information from a gff file and return a dictionary of locus tag features.\n",
    "    \"\"\"\n",
    "    # Create an in-memory database from the GFF file\n",
    "    gff_db = gffutils.create_db(gff_file, ':memory:', merge_strategy='merge')\n",
    "    gene_info = {}\n",
    "\n",
    "    # extract taxon_id from the gff file name (first element of _ separated string)\n",
    "    taxon_id = os.path.basename(gff_file).split('_')[0]\n",
    "\n",
    "    # extract the accession ID from the gff db header\n",
    "    # read the file line by line and extract the accession ID from the line starting with '#!genome-build-accession'\n",
    "    with open(gff_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#!genome-build-accession'):\n",
    "                accession_id = line.split(':')[1].strip()\n",
    "                break\n",
    "\n",
    "\n",
    "    # Iterate over each feature of type 'gene' in the GFF database\n",
    "    for feature in gff_db.features_of_type('gene'):\n",
    "        # Get the locus tags and old locus tags from the feature attributes\n",
    "        locus_tags = feature.attributes.get('locus_tag', [])\n",
    "        old_locus_tags = feature.attributes.get('old_locus_tag', [])\n",
    "        # Combine locus tags and old locus tags into a single set to avoid duplicates\n",
    "        all_locus_tags = set(locus_tags + old_locus_tags)\n",
    "        # Iterate over each locus tag in the combined set\n",
    "        for locus_tag in all_locus_tags:\n",
    "            # Check if the locus tag is in the list of found gene IDs\n",
    "            if locus_tag in taxon_gene_ids_df['locus_tag'].values:\n",
    "                # If yes, add the gene information to the gene_info dictionary\n",
    "                gene_info[locus_tag] = {\n",
    "                    'id': feature.id,  # Gene ID\n",
    "                    'start': feature.start,  # Start position of the gene\n",
    "                    'end': feature.end,  # End position of the gene\n",
    "                    'strand': feature.strand,  # Strand information\n",
    "                    'attributes': dict(feature.attributes),  # Feature attributes\n",
    "                    'seqid': feature.seqid,  # Sequence ID\n",
    "                    'accession_id': accession_id  # Genome accession ID\n",
    "                }\n",
    "\n",
    "    return taxon_id, gene_info\n",
    "\n",
    "def extract_gene_info_from_gff_wrapper(args):\n",
    "    return extract_gene_info_from_gff(*args)\n",
    "\n",
    "# Initialize the all_gene_info dictionary\n",
    "all_gene_info = {}\n",
    "extract_gene_info_from_gff_args = [(gff_file, found_gene_ids_df[found_gene_ids_df['taxon_id'] == os.path.basename(gff_file).split('_')[0]]) for gff_file in gff_files]\n",
    "# Extract gene information from all GFF files in parallel\n",
    "num_gff_files = len(gff_files)\n",
    "with Pool(mp.cpu_count()) as pool:\n",
    "    results = pool.starmap(extract_gene_info_from_gff, extract_gene_info_from_gff_args)\n",
    "    for taxon_id, gene_info in tqdm.tqdm(results, total=num_gff_files):\n",
    "        all_gene_info[taxon_id] = gene_info\n",
    "\n",
    "# Save the gene information to a JSON file\n",
    "gene_info_json_filepath = os.path.join(filtered_dir, 'gene_features.json')\n",
    "with open(gene_info_json_filepath, 'w') as json_file:\n",
    "    json.dump(all_gene_info, json_file, indent=4)\n",
    "\n",
    "print(f\"Extracted gene information for {len(all_gene_info)} genes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare subset of EggNOG gene trees and alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NOGs after filtering, with at least 4 taxa and genes with available GFF files: 8197\n",
      "Number of single copy NOGs: 3718\n"
     ]
    }
   ],
   "source": [
    "# Create a set of found locus tags for faster lookup\n",
    "found_locus_tags_set = set(found_locus_tags)\n",
    "\n",
    "# Create a dictionary for faster lookup of gene_id to locus_tag\n",
    "gene_id_to_locus_tag = {gene_id: locus_tag for gene_id, locus_tag in zip(\n",
    "    gene_ids_df['gene_id'], gene_ids_df['locus_tag'])}\n",
    "\n",
    "# Filter out genes that are not in the found_locus_tags set, from the nog2taxa_df members column\n",
    "nog2taxa_df['filtered_members'] = nog2taxa_df['filtered_members'].apply(\n",
    "    lambda members_list: [member for member in members_list if gene_id_to_locus_tag.get(member, '') in found_locus_tags_set])\n",
    "nog2taxa_df['filtered_members_count'] = nog2taxa_df['filtered_members'].apply(\n",
    "    len)\n",
    "\n",
    "# similarly for the taxa column\n",
    "nog2taxa_df['filtered_taxa'] = nog2taxa_df['filtered_members'].apply(\n",
    "    lambda members_list: list(set([member.split('.')[0] for member in members_list])))\n",
    "nog2taxa_df['filtered_taxa_count'] = nog2taxa_df['filtered_taxa'].apply(len)\n",
    "\n",
    "# keep only nogs with at least 4 taxa\n",
    "nog2taxa_df = nog2taxa_df[nog2taxa_df['filtered_taxa_count'] >= 4]\n",
    "print(f\"Number of NOGs after filtering, with at least 4 taxa and genes with available GFF files: {\n",
    "      nog2taxa_df.shape[0]}\")\n",
    "\n",
    "# rearrange to have the counts before the lists\n",
    "nog2taxa_df = nog2taxa_df[['level', 'nog', 'filtered_taxa_count',\n",
    "                           'filtered_members_count', 'filtered_taxa', 'filtered_members']]\n",
    "\n",
    "# make the lists as comma-separated strings\n",
    "nog2taxa_df['filtered_taxa'] = nog2taxa_df['filtered_taxa'].apply(\n",
    "    lambda taxa_list: ','.join(taxa_list))\n",
    "nog2taxa_df['filtered_members'] = nog2taxa_df['filtered_members'].apply(\n",
    "    lambda members_list: ','.join(members_list))\n",
    "\n",
    "# to file\n",
    "nog2taxa_filtered_filepath = os.path.join(\n",
    "    filtered_dir, 'map.nog_taxa_members.tsv')\n",
    "nog2taxa_df.to_csv(nog2taxa_filtered_filepath, sep='\\t', index=False)\n",
    "# make a list of the filtered nogs\n",
    "filtered_nogs_filepath = os.path.join(filtered_dir, 'nogs.taxa_filtered.gff_filtered.txt')\n",
    "nog2taxa_df['nog'].to_csv(filtered_nogs_filepath, index=False, header=False)\n",
    "\n",
    "# prepare a NOG to members list map for all the NOGs that have only single copy genes (i.e. filtered_members_count == filtered_taxa_count)\n",
    "single_copy_nogs2members_df = nog2taxa_df[nog2taxa_df['filtered_members_count'].astype(int) == nog2taxa_df['filtered_taxa_count'].astype(int)]\n",
    "single_copy_nogs2members_df = single_copy_nogs2members_df[['nog', 'filtered_members']]\n",
    "# write to file\n",
    "single_copy_nogs2members_filepath = os.path.join(filtered_dir, 'map.nog_members_single_copy.tsv')\n",
    "single_copy_nogs2members_df.to_csv(single_copy_nogs2members_filepath, sep='\\t', index=False, header=False)\n",
    "print(f\"Number of single copy NOGs: {single_copy_nogs2members_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we don't have gene features for all genes in this list of NOGs, we know which taxa are present in them and we have gene trees estimated for them in EggNOG. We therefore use the original GOLD-EggNOG overlap list of NOGs and find the ones that are broadest in terms of representing all the taxa we have gene features for - to be used for genome tree inference. This means that the HGT inference we make later will be based on the set of gene trees that we have features for, but the genome tree will be based on a set of gene trees regardless of whether we have features for them or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NOGs with at least 4 of the filtered taxa: 8197\n",
      "Number of NOGs where each of them have at least 0.95 of all filtered taxa (208 out of 219 taxa): 363\n"
     ]
    }
   ],
   "source": [
    "# get NOGs that have all or almost all sampled taxa of GOLD-E6 overlap (for genome tree inference)\n",
    "# read in original NOG2taxa file for the gold-e6 overlap\n",
    "nog2taxa_filepath = os.path.join(e6_dir, 'e6.og2seqs_and_species_bacteria.tsv')\n",
    "nog2taxa_df = pd.read_csv(nog2taxa_filepath, sep='\\t', header=None, names=['level', 'nog', 'taxa_count', 'members_count', 'taxa', 'members'])\n",
    "# remove all the NOGs not in nogs.taxa_filtered.gff_filtered.txt\n",
    "filtered_nogs_filepath = os.path.join(filtered_dir, 'nogs.taxa_filtered.gff_filtered.txt')\n",
    "with open(filtered_nogs_filepath, 'r') as file:\n",
    "    filtered_nogs = file.read().splitlines()\n",
    "# read in filtered taxa list\n",
    "filtered_taxa_filepath = os.path.join(gold_e6_overlap_dir, 'e6_gold.filtered_taxa.txt')\n",
    "filtered_taxa_df = pd.read_csv(filtered_taxa_filepath, sep='\\t', names=['taxon_ID'], dtype={'taxon_ID': str})\n",
    "filtered_taxa_list = filtered_taxa_df['taxon_ID'].tolist()\n",
    "nog2taxa_df = nog2taxa_df[nog2taxa_df['nog'].isin(filtered_nogs)]\n",
    "# keep only taxa that are in the filtered taxa list\n",
    "nog2taxa_df['taxa'] = nog2taxa_df['taxa'].apply(lambda taxa_list: taxa_list.split(','))\n",
    "nog2taxa_df['taxa'] = nog2taxa_df['taxa'].apply(lambda taxa_list: [taxon for taxon in taxa_list if taxon in filtered_taxa_list])\n",
    "nog2taxa_df['taxa_count'] = nog2taxa_df['taxa'].apply(len)\n",
    "\n",
    "# keep only NOGs that have at least 4 taxa\n",
    "nog2taxa_df = nog2taxa_df[nog2taxa_df['taxa_count'] >= 4]\n",
    "print(f\"Number of NOGs with at least 4 of the filtered taxa: {nog2taxa_df.shape[0]}\")\n",
    "\n",
    "# first, get NOGs that have all or almost all sampled taxa of GOLD-E6 overlap. This is a small subset of NOGs\n",
    "sampling_fraction = 0.95 # at least this fraction of the sampled taxa should be present in the NOG\n",
    "sampling_taxa_count = int(sampling_fraction * len(filtered_taxa_list))\n",
    "broad_distribution_nogs = nog2taxa_df[nog2taxa_df['taxa_count'] >= sampling_taxa_count].copy()\n",
    "print(f\"Number of NOGs where each of them have at least {sampling_fraction} of all filtered taxa ({\n",
    "    sampling_taxa_count} out of {len(filtered_taxa_list)} taxa):\", broad_distribution_nogs.shape[0])\n",
    "# # sampling_fraction = 1.0\n",
    "# broad_distribution_nogs = broad_distribution_nogs[broad_distribution_nogs['taxa_count'] == len(filtered_taxa_list)].copy()\n",
    "# print(f\"Number of NOGs where each of them have all filtered taxa ({len(filtered_taxa_list)} taxa):\", broad_distribution_nogs.shape[0])\n",
    "\n",
    "# write nogs list to file\n",
    "broad_distribution_nogs_filepath = os.path.join(filtered_dir, 'nogs.broad_distribution.txt')\n",
    "broad_distribution_nogs['nog'].to_csv(broad_distribution_nogs_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nog2taxa file for NOGs with at least 4 taxa\n",
    "nog2taxa_filepath = os.path.join(filtered_dir, 'map.nog_taxa_members.tsv')\n",
    "nog2taxa_df = pd.read_csv(nog2taxa_filepath, sep='\\t')\n",
    "# find how many NOGs have single copy orthologs in all taxa\n",
    "single_copy_nogs = nog2taxa_df[nog2taxa_df['filtered_members_count'] == len(filtered_taxa_list)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was in the file mapping NOGs to constituent taxa or gene members. Now get a subset of trees and alignments that only contain the chosen NOGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogs_chosen_path = f\"{filtered_dir}/nogs.taxa_filtered.gff_filtered.txt\"\n",
    "all_trees_bacteria_path = f\"{e6_dir}/e6.all_raw_trees_and_algs_bacteria.tsv\"\n",
    "\n",
    "# filepaths for the subset of the E6 trees/algs to be used in the analysis\n",
    "trees_plus_algs_path = f\"{filtered_dir}/map.nog_tree_alg.filtered.tsv\"\n",
    "trees_path = f\"{filtered_dir}/map.nog_tree.filtered.tsv\"\n",
    "algs_path = f\"{filtered_dir}/map.nog_alg.filtered.tsv\"\n",
    "nogs_with_trees_path = f\"{filtered_dir}/nogs.filtered.with_trees.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees in the output file: 8186 /root/work/projects/hgt_ecosystem/data/filtered/map.nog_tree.filtered.tsv\n",
      "Number of NOGs chosen (line count including header): 8197 /root/work/projects/hgt_ecosystem/data/filtered/nogs.taxa_filtered.gff_filtered.txt\n",
      "NOGs without trees:\n",
      "COG0236\n",
      "COG0318\n",
      "COG0438\n",
      "COG0457\n",
      "COG0515\n",
      "COG0553\n",
      "COG0642\n",
      "COG0745\n",
      "COG0784\n",
      "COG1028\n",
      "COG2202\n",
      "COG2814\n",
      "COG2931\n",
      "COG3210\n",
      "D9ST1\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$ogs_chosen_path\" \"$all_trees_bacteria_path\" \"$trees_plus_algs_path\" \"$trees_path\" \"$algs_path\" \"$nogs_with_trees_path\"\n",
    "cut -f3 $1 |tail -n +2 | rg -wFf - $2 > $3 # get list of chosen NOGs\n",
    "# Separate trees in newick format from encoded alignments\n",
    "cut -f1,3 $3 > $4 # extract trees from temp file\n",
    "cut -f1,4 $3 > $5 # extract alignments from temp file\n",
    "# rm $3\n",
    "# Note that some of the trees don't exist in eggNOG for some of the NOGs that we chose.\n",
    "# print the number of trees that are there in the trees_path file and the number of NOGs chosen\n",
    "echo \"Number of trees in the output file: $(wc -l $4)\"\n",
    "echo \"Number of NOGs chosen (line count including header): $(wc -l $1)\"\n",
    "# print the NOG IDs that are there in 3rd column of chosen NOGs file but not in the 1st column of the trees_path file\n",
    "# basically the difference between those two columns from the two files\n",
    "echo \"NOGs without trees:\"\n",
    "comm -23 <(cut -f3 $1 |tail -n +2 | sort) <(cut -f1 $4 |tail -n +2 | sort)\n",
    "\n",
    "# make a list of NOGs that have trees\n",
    "cut -f1 $4 |tail -n +2 > $6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune the gene trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8182/8182 [00:37<00:00, 219.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pruned gene trees: 8182 out of 8182\n"
     ]
    }
   ],
   "source": [
    "# first we prune the gene trees to be used for genome tree inference\n",
    "# we keep only the taxa that are in the filtered taxa list\n",
    "\n",
    "# Read the gene trees\n",
    "gene_trees_path = f\"{filtered_dir}/map.nog_tree.filtered.tsv\"\n",
    "gene_trees_df = pd.read_csv(gene_trees_path, header=None, index_col=None, sep='\\t', names=['nog','tree'])\n",
    "\n",
    "# read in the filtered nog2taxa file\n",
    "nog2taxa_filtered_filepath = os.path.join(filtered_dir, 'map.nog_taxa_members.tsv')\n",
    "nog2taxa_filtered_df = pd.read_csv(nog2taxa_filtered_filepath, sep='\\t', header=0)\n",
    "\n",
    "# we prune the gene trees to keep only the members that are in the filtered_members column of the nog2taxa file for each NOG\n",
    "def prune_gene_tree_using_genes_list(args):\n",
    "    nog, tree, genes_to_keep = args\n",
    "    # read in the tree\n",
    "    tree = ete3.Tree(tree, format=1)\n",
    "    # prune the tree\n",
    "    tree.prune(genes_to_keep)\n",
    "    # check if the tree has at least 4 taxa\n",
    "    taxa_in_tree = set([leaf.name.split('.')[0] for leaf in tree.get_leaves()])\n",
    "    if len(taxa_in_tree) < 4:\n",
    "        print(f\"Tree {nog} has less than 4 taxa after pruning.\")\n",
    "        return None\n",
    "    else:\n",
    "        return nog, tree.write(format=1)\n",
    "    \n",
    "# make a three column df with NOG, tree and genes to keep\n",
    "gene_trees_to_prune_df = pd.merge(gene_trees_df, nog2taxa_filtered_df[['nog', 'filtered_members']], how='inner', left_on='nog', right_on='nog')\n",
    "gene_trees_to_prune_df['filtered_members'] = gene_trees_to_prune_df['filtered_members'].str.split(',')\n",
    "gene_trees_to_prune_df = gene_trees_to_prune_df[['nog', 'tree', 'filtered_members']]\n",
    "# convert the df to a list of tuples\n",
    "gene_trees_to_prune_df = gene_trees_to_prune_df.values\n",
    "\n",
    "# prune the gene trees in parallel using imap_unordered\n",
    "gene_trees_pruned = {}\n",
    "with Pool(mp.cpu_count()) as pool:\n",
    "    for result in tqdm.tqdm(pool.imap_unordered(prune_gene_tree_using_genes_list, gene_trees_to_prune_df), total=len(gene_trees_to_prune_df)):\n",
    "        if result is not None:\n",
    "            nog, tree = result\n",
    "            gene_trees_pruned[nog] = tree\n",
    "\n",
    "# save the pruned gene trees to a file\n",
    "gene_trees_pruned_filepath = os.path.join(filtered_dir, 'map.nog_tree.filtered.pruned.tsv')\n",
    "with open(gene_trees_pruned_filepath, 'w') as file:\n",
    "    file.write('\\n'.join([f'{nog}\\t{tree}' for nog, tree in gene_trees_pruned.items()]))\n",
    "    file.write('\\n')\n",
    "\n",
    "# print the number of pruned gene trees\n",
    "print(f\"Number of pruned gene trees: {len(gene_trees_pruned)} out of {len(gene_trees_to_prune_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$filtered_dir\" \"$all_trees_bacteria_path\" \n",
    "# now we prune the gene trees to be used for genome tree inference\n",
    "# first we grep the trees for these broad distribution NOGs\n",
    "rg -wFf $1/nogs.broad_distribution.txt $2 > $1/map.nog_tree_alg.broad_distribution.unpruned.tsv\n",
    "# separate trees in newick format from encoded alignments\n",
    "cut -f1,3 $1/map.nog_tree_alg.broad_distribution.unpruned.tsv > $1/map.nog_tree.broad_distribution.unpruned.tsv\n",
    "cut -f1,4 $1/map.nog_tree_alg.broad_distribution.unpruned.tsv > $1/map.nog_alg.broad_distribution.unpruned.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [00:17<00:00, 13.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pruned gene trees: 233 out of 233\n",
      "Number of genes in the broad distribution NOGs: 71906\n",
      "Sample of the gene to taxa mapping:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>taxon_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>546263.NELON_07490</td>\n",
       "      <td>546263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238.BBD35_03635</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>487316.BEN76_15855</td>\n",
       "      <td>487316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106654.B7L44_03485</td>\n",
       "      <td>106654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471.BUM88_16755</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              gene_id taxon_id\n",
       "0  546263.NELON_07490   546263\n",
       "0     238.BBD35_03635      238\n",
       "0  487316.BEN76_15855   487316\n",
       "0  106654.B7L44_03485   106654\n",
       "0     471.BUM88_16755      471"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in the unpruned gene trees\n",
    "unpruned_gene_trees_path = f'{filtered_dir}/map.nog_tree.broad_distribution.unpruned.tsv'\n",
    "unpruned_gene_trees_df = pd.read_csv(unpruned_gene_trees_path, header=None, index_col=None, sep='\\t', names=['nog','tree'])\n",
    "\n",
    "# read in the filtered taxa list\n",
    "filtered_taxa_filepath = os.path.join(filtered_dir, 'taxa.overlap_filtered.gff_filtered.txt')\n",
    "with open(filtered_taxa_filepath, 'r') as file:\n",
    "    filtered_taxa = file.read().splitlines()\n",
    "\n",
    "def prune_gene_tree_using_taxa_list(args):\n",
    "    nog, tree, taxa_to_keep = args\n",
    "    # read in the tree\n",
    "    tree = ete3.Tree(tree, format=1)\n",
    "    # read in the leaves of the tree and make a list of leaves to keep\n",
    "    # these are the leaves that have first element of the period separated leaf name in the taxa_to_keep list\n",
    "    leaves_to_keep = [leaf for leaf in tree.get_leaves() if leaf.name.split('.')[0] in taxa_to_keep]\n",
    "    # prune the tree\n",
    "    tree.prune(leaves_to_keep)\n",
    "    # check if the tree has at least 4 taxa\n",
    "    taxa_in_tree = set([leaf.name.split('.')[0] for leaf in tree.get_leaves()])\n",
    "    if len(taxa_in_tree) < 4:\n",
    "        print(f\"Tree {nog} has less than 4 taxa after pruning.\")\n",
    "        return None\n",
    "    else:\n",
    "        return nog, tree.write(format=1), taxa_to_keep, leaves_to_keep\n",
    "    \n",
    "# make a three column df with NOG, tree and taxa to keep\n",
    "unpruned_gene_trees_to_prune_df = pd.DataFrame(unpruned_gene_trees_df)\n",
    "unpruned_gene_trees_to_prune_df['taxa_to_keep'] = [filtered_taxa] * len(unpruned_gene_trees_to_prune_df)\n",
    "unpruned_gene_trees_to_prune_df = unpruned_gene_trees_to_prune_df[['nog', 'tree', 'taxa_to_keep']]\n",
    "# convert the df to a list of tuples\n",
    "unpruned_gene_trees_to_prune_df = unpruned_gene_trees_to_prune_df.values\n",
    "\n",
    "# prune the gene trees in parallel using imap_unordered\n",
    "gene_trees_pruned = {}\n",
    "broad_dist_nog_info = {}\n",
    "with Pool(mp.cpu_count()) as pool:\n",
    "    for result in tqdm.tqdm(pool.imap_unordered(prune_gene_tree_using_taxa_list, unpruned_gene_trees_to_prune_df), total=len(unpruned_gene_trees_to_prune_df)):\n",
    "        if result is not None:\n",
    "            nog, pruned_tree, taxa_to_keep, leaves_to_keep = result\n",
    "            gene_trees_pruned[nog] = pruned_tree\n",
    "            broad_dist_nog_info[nog] = {'taxa_count': len(taxa_to_keep), 'members_count': len(leaves_to_keep), \n",
    "                                        'taxa': ','.join(taxa_to_keep), 'members': ','.join([leaf.name for leaf in leaves_to_keep])}\n",
    "\n",
    "# save the pruned gene trees to a file\n",
    "gene_trees_pruned_filepath = os.path.join(filtered_dir, 'map.nog_tree.broad_distribution.pruned.tsv')\n",
    "with open(gene_trees_pruned_filepath, 'w') as file:\n",
    "    file.write('\\n'.join([f'{nog}\\t{tree}' for nog, tree in gene_trees_pruned.items()]))\n",
    "    file.write('\\n')\n",
    "\n",
    "# write the second column (just the trees) to a file\n",
    "broad_dist_gene_trees_pruned_filepath = os.path.join(filtered_dir, 'gene_trees.broad_distribution.pruned.nwk')\n",
    "with open(broad_dist_gene_trees_pruned_filepath, 'w') as file:\n",
    "    file.write('\\n'.join([tree for tree in gene_trees_pruned.values()]) + '\\n')\n",
    "\n",
    "# save the broad distribution NOG info to a file\n",
    "broad_dist_nog_info_filepath = os.path.join(filtered_dir, 'map.nog_taxa_members.broad_distribution.tsv')\n",
    "broad_dist_nog_info_df = pd.DataFrame.from_dict(broad_dist_nog_info, orient='index')\n",
    "broad_dist_nog_info_df.index.name = 'nog'\n",
    "broad_dist_nog_info_df.reset_index(inplace=True)\n",
    "broad_dist_nog_info_df.to_csv(broad_dist_nog_info_filepath, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "# print the number of pruned gene trees\n",
    "print(f\"Number of pruned gene trees: {len(gene_trees_pruned)} out of {len(unpruned_gene_trees_to_prune_df)}\")\n",
    "\n",
    "# prepare a mapping file for astral, which maps genes to taxa\n",
    "# this is a TSV file with two columns, the first column is a gene ID (e.g. 1234.ABC_789) and the second column is the taxon ID (e.g. 1234)\n",
    "# we extract the members column from the broad distribution info df and explode to get a list of all gene IDs\n",
    "broad_dist_gene_taxa_mapping_df = broad_dist_nog_info_df['members'].str.split(',', expand=False).explode().to_frame()\n",
    "broad_dist_gene_taxa_mapping_df.columns = ['gene_id']\n",
    "broad_dist_gene_taxa_mapping_df['taxon_id'] = broad_dist_gene_taxa_mapping_df['gene_id'].apply(lambda x: x.split('.')[0])\n",
    "broad_dist_gene_taxa_mapping_df.to_csv(os.path.join(filtered_dir, 'map.gene_taxa.broad_distribution.txt'),\n",
    "                                       sep=' ', index=False, header=False)\n",
    "\n",
    "print(f\"Number of genes in the broad distribution NOGs: {broad_dist_gene_taxa_mapping_df.shape[0]}\")\n",
    "print(\"Sample of the gene to taxa mapping:\")\n",
    "display(broad_dist_gene_taxa_mapping_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 233 compressed alignments.\n",
      "Example taxa with available GFF files: ['587', '29542', '43767', '47917', '69370']\n",
      "Processing 233 alignments in parallel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pruning alignments: 100%|██████████| 233/233 [01:13<00:00,  3.15it/s]\n",
      "Converting fasta to nexus: 100%|██████████| 233/233 [00:17<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Nexus file written to /root/work/projects/hgt_ecosystem/data/filtered/algs.filtered.concatenated.nex\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the alignments for IQTree.\n",
    "# However, they have been compressed, in `map.nog_alg.broad_distribution.unpruned.tsv` so we need to decompress them first.\n",
    "# Furthermore, these alignments contain all members that were present in the OG, but since we pruned the trees we need to prune the alignments as well.\n",
    "# At the same time, we decide (at random) which paralog gets chosen as representative for the taxon ID, the others get removed.\n",
    "import gzip\n",
    "import base64\n",
    "from io import StringIO\n",
    "from Bio import AlignIO\n",
    "from Bio.Nexus import Nexus\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "alg_dir = os.path.join(filtered_dir, 'broad_distribution_algs')\n",
    "if os.path.exists(alg_dir):\n",
    "    print(f\"Directory {alg_dir} already exists. Removing it.\")\n",
    "    shutil.rmtree(alg_dir)\n",
    "os.makedirs(alg_dir)\n",
    "\n",
    "filtered_taxa_w_available_gff_filepath = os.path.join(\n",
    "    filtered_dir, 'taxa.overlap_filtered.gff_filtered.txt')\n",
    "# read it in as a pd series\n",
    "filtered_taxa_w_available_gff = pd.read_csv(filtered_taxa_w_available_gff_filepath, header=None, names=['taxon_id'], dtype=str)['taxon_id']\n",
    "\n",
    "def convert_fasta_to_nexus(in_file: str, out_file: str, out_format: str = 'nexus'):\n",
    "    # It works here, but not if I do it outside the function :)\n",
    "    AlignIO.convert(in_file=in_file, in_format='fasta', out_file=out_file,\n",
    "                    out_format=out_format, molecule_type='protein')\n",
    "    return 0\n",
    "\n",
    "\n",
    "def remove_paralogous_duplicate_taxa(alignments: list) -> set:\n",
    "    \"\"\"\n",
    "    Removes paralogs from the alignment by choosing one member at random as representative for the taxon ID.\n",
    "    Takes a list of SeqRecords as input.\n",
    "    Returns alignment ids that are to be kept, the rest get dropped in another step.\n",
    "    \"\"\"\n",
    "    taxa = {alignment.id.split('.')[0]: [] for alignment in alignments}\n",
    "    for alignment in alignments:\n",
    "        taxa[alignment.id.split('.')[0]].append(alignment.id)\n",
    "    sample = {np.random.choice(taxa[gene]) for gene in taxa}\n",
    "    return sample\n",
    "\n",
    "############################################################################################################################################################################\n",
    "\n",
    "\n",
    "# Read in aligments\n",
    "algs = pd.read_csv(os.path.join(filtered_dir, 'map.nog_alg.broad_distribution.unpruned.tsv'),\n",
    "                   header=None, names=['og', 'algs'], sep='\\t')\n",
    "print(f\"Read in {algs.shape[0]} compressed alignments.\")\n",
    "\n",
    "\n",
    "def process_alignment(args):\n",
    "    \n",
    "    alg, taxa_set = args\n",
    "    og_name = alg['og']\n",
    "    compressed_algs = alg['algs']\n",
    "    fasta_file_path = os.path.join(alg_dir, f'{og_name}.fasta')\n",
    "\n",
    "    # Skip if the file already exists\n",
    "    if os.path.exists(fasta_file_path):\n",
    "        print(f\"File {fasta_file_path} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Decompress the file\n",
    "    fasta = StringIO(gzip.decompress(base64.b64decode(compressed_algs)).decode())\n",
    "\n",
    "    # Read the alignment\n",
    "    alignment = AlignIO.read(fasta, format='fasta')\n",
    "\n",
    "    # Retain only the taxa that are present in the pruned tree\n",
    "    pruned_alignment = [record for record in alignment if record.id.split('.')[0] in taxa_set]\n",
    "\n",
    "    # Remove duplicates of taxa if paralogs are present\n",
    "    ids_to_keep = remove_paralogous_duplicate_taxa(pruned_alignment)\n",
    "    pruned_alignment_without_duplicates = [record for record in pruned_alignment if record.id in ids_to_keep]\n",
    "\n",
    "    # Write each alignment in a separate file per OG\n",
    "    with open(fasta_file_path, 'w') as f:\n",
    "        for record in pruned_alignment_without_duplicates:\n",
    "            f.write(f'>{record.id.split(\".\")[0]}\\n{record.seq}\\n')\n",
    "\n",
    "algs_list = algs.to_dict(orient='records')\n",
    "filtered_taxa_w_available_gff_set = set(filtered_taxa_w_available_gff.astype(str).tolist())\n",
    "print(f\"Example taxa with available GFF files: {list(filtered_taxa_w_available_gff_set)[:5]}\")\n",
    "algs_args = [(alg, filtered_taxa_w_available_gff_set) for alg in algs_list]\n",
    "print(f\"Processing {len(algs_args)} alignments in parallel.\")\n",
    "\n",
    "# Use multiprocessing to process alignments in parallel\n",
    "with Pool(mp.cpu_count() -2) as pool:\n",
    "    list(tqdm.tqdm(pool.imap_unordered(process_alignment, algs_args), total=len(algs), desc='Pruning alignments'))\n",
    "\n",
    "\n",
    "############################################################################################################################################################################\n",
    "\n",
    "# Convert fasta to nexus if the nexus file doesn't exist or is older than the fasta file\n",
    "nex_file_list = []\n",
    "for fasta_file in tqdm.tqdm(glob.glob(os.path.join(os.path.expanduser(alg_dir), '*.fasta')), desc='Converting fasta to nexus'):\n",
    "    # get basename without extension\n",
    "    fasta_file_basename_root = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "    nexus_file = os.path.join(alg_dir, f'{fasta_file_basename_root}.nex')\n",
    "\n",
    "    # If the nexus file doesn't exist or is older than the fasta file, convert the fasta file\n",
    "    if not os.path.exists(nexus_file) or os.path.getmtime(fasta_file) > os.path.getmtime(nexus_file):\n",
    "        convert_fasta_to_nexus(fasta_file, nexus_file, 'nexus')\n",
    "    else:\n",
    "        print(f\"File {nexus_file} is up-to-date. Skipping conversion.\")\n",
    "\n",
    "    nex_file_list.append(nexus_file)\n",
    "\n",
    "# Concatenate Nexus files and write to a new file\n",
    "nexi = [(fname, Nexus.Nexus(fname)) for fname in nex_file_list]\n",
    "combined = Nexus.combine(nexi)\n",
    "with open(os.path.join(filtered_dir, 'algs.filtered.concatenated.nex'), \"w\") as output_file:\n",
    "    combined.write_nexus_data(filename=output_file, interleave=False)\n",
    "print(f\"Concatenated Nexus file written to {os.path.join(filtered_dir, 'algs.filtered.concatenated.nex')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Presence-Absence matrices \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## For NOGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the pa_df is: (159, 8197)\n",
      "Presence-absence matrix written to /root/work/projects/hgt_ecosystem/data/filtered/pa_matrix.nogs.binary.fasta\n"
     ]
    }
   ],
   "source": [
    "# make presence absence matrix for the full set of NOGs (not just the broad distribution ones)\n",
    "# read in the filtered nog2taxa file as the file on members in the NOGs\n",
    "nog2taxa_filtered_filepath = os.path.join(filtered_dir, 'map.nog_taxa_members.tsv')\n",
    "nog2taxa_filtered_df = pd.read_csv(nog2taxa_filtered_filepath, sep='\\t', header=0)\n",
    "\n",
    "# break the taxa column into a list of taxa\n",
    "nog2taxa_filtered_df['filtered_taxa'] = nog2taxa_filtered_df['filtered_taxa'].apply(\n",
    "    lambda x: x.split(','))\n",
    "nog2taxa_filtered_df['filtered_members'] = nog2taxa_filtered_df['filtered_members'].apply(\n",
    "    lambda x: x.split(','))\n",
    "\n",
    "# create a pa matrix for the full set of NOGs\n",
    "# first create a list of all the taxa\n",
    "taxa = list(\n",
    "    set([item for sublist in nog2taxa_filtered_df['filtered_taxa'].tolist() for item in sublist]))\n",
    "# sort the list\n",
    "taxa.sort()\n",
    "\n",
    "# create a dataframe with the NOGs as columns and the taxa as index\n",
    "pa_df = pd.DataFrame(index=taxa, columns=nog2taxa_filtered_df['nog'], dtype=str).fillna('0')\n",
    "for index, row in nog2taxa_filtered_df.iterrows():\n",
    "    for taxon in row['filtered_taxa']:\n",
    "        pa_df.loc[taxon, row['nog']] = str(int(len([gene for gene in row['filtered_members'] if gene.split('.')[0] == taxon])))\n",
    "\n",
    "# display the size of the matrix/df\n",
    "print(\"The size of the pa_df is: {}\".format(pa_df.shape))\n",
    "\n",
    "# TSV file for COUNT, with NOGs in index and taxa as columns\n",
    "pa_df.T.to_csv(f\"{filtered_dir}/pa_matrix.nogs.numerical.tsv\", sep='\\t')\n",
    "\n",
    "# now binarize the matrix (i.e. if a NOG has at least one gene from a taxon, it gets a 1, otherwise 0)\n",
    "pa_df = pa_df.map(lambda x: '1' if x != '0' else '0')\n",
    "\n",
    "# Concatenate the columns into a single string per taxon and write out as FASTA file\n",
    "fasta_df = pd.DataFrame(pa_df.astype(str).apply(''.join, axis=1), columns=['sequence'])\n",
    "fasta_df.index = '>' + fasta_df.index.str.strip()\n",
    "fasta_df_filepath = f\"{filtered_dir}/pa_matrix.nogs.binary.fasta\"\n",
    "fasta_df.to_csv(f\"{fasta_df_filepath}\",\n",
    "                sep='\\n', header=False)\n",
    "\n",
    "print(f\"Presence-absence matrix written to {fasta_df_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Ecosystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxa in the taxon-ecosystem map that was read in: 219\n",
      "Number of taxa in the taxon-ecosystem map after filtering for ones with available GFF files: 159\n",
      "Presence-absence matrices written out.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# read in the list of taxa after filtering for overlap and GFF files\n",
    "taxa_filepath = os.path.join(filtered_dir, 'taxa.overlap_filtered.gff_filtered.txt')\n",
    "with open(taxa_filepath, 'r') as file:\n",
    "    filtered_taxa_list = file.read().splitlines()\n",
    "\n",
    "# read in the tsv file for df mapping taxon IDs to ecosystem labels\n",
    "taxon_ecosystem_map_filepath = os.path.join(data_dir, 'gold_e6_overlap/e6_gold.overlap_subset_df.tsv')\n",
    "taxon_ecosystem_map_df = pd.read_csv(taxon_ecosystem_map_filepath, sep='\\t', header=0)\n",
    "print(f\"Number of taxa in the taxon-ecosystem map that was read in: {taxon_ecosystem_map_df['taxon_ID'].nunique()}\")\n",
    "\n",
    "# filter the taxon_ecosystem_map_df to keep only the taxa that are in the filtered_taxa_list\n",
    "taxon_ecosystem_map_df['taxon_ID'] = taxon_ecosystem_map_df['taxon_ID'].astype(str)\n",
    "taxon_ecosystem_map_df = taxon_ecosystem_map_df[taxon_ecosystem_map_df['taxon_ID'].isin(filtered_taxa_list)]\n",
    "print(f\"Number of taxa in the taxon-ecosystem map after filtering for ones with available GFF files: {taxon_ecosystem_map_df['taxon_ID'].nunique()}\")\n",
    "\n",
    "# there are two kinds of pa-matrices to make: \n",
    "# one at the 'ORGANISM ECOSYSTEM TYPE' level and one at the 'ORGANISM ECOSYSTEM SUBTYPE' level (both are columns in the df)\n",
    "taxon_ecosystem_type_df = taxon_ecosystem_map_df[['taxon_ID', 'ORGANISM ECOSYSTEM TYPE']]\n",
    "taxon_ecosystem_subtype_df = taxon_ecosystem_map_df[['taxon_ID', 'ORGANISM ECOSYSTEM SUBTYPE']]\n",
    "\n",
    "# take counts of taxon,ecosystem_type pairs and similarly for taxon,ecosystem_subtype pairs\n",
    "taxon_ecosystem_type_counts_df = taxon_ecosystem_type_df.groupby(['taxon_ID', 'ORGANISM ECOSYSTEM TYPE']).size().reset_index(name='counts').astype(str)\n",
    "taxon_ecosystem_subtype_counts_df = taxon_ecosystem_subtype_df.groupby(['taxon_ID', 'ORGANISM ECOSYSTEM SUBTYPE']).size().reset_index(name='counts').astype(str)\n",
    "\n",
    "# pivot the dfs to get the pa matrices\n",
    "taxon_ecosystem_type_pa_df = taxon_ecosystem_type_counts_df.pivot(columns='taxon_ID', index='ORGANISM ECOSYSTEM TYPE', values='counts').fillna('0').astype(str)\n",
    "taxon_ecosystem_subtype_pa_df = taxon_ecosystem_subtype_counts_df.pivot(columns='taxon_ID', index='ORGANISM ECOSYSTEM SUBTYPE', values='counts').fillna('0').astype(str)\n",
    "\n",
    "ecotype_index_names = taxon_ecosystem_type_pa_df.index.tolist()\n",
    "# convert all spaces,hyphens,slashes to underscores in the index names\n",
    "ecotype_index_names = {index:str(re.sub(r'[\\s/-]', '_', index)) for index in ecotype_index_names}\n",
    "\n",
    "# rename the index names\n",
    "taxon_ecosystem_type_pa_df.rename(index=ecotype_index_names, inplace=True)\n",
    "\n",
    "# similarly for the ecosystem subtype df\n",
    "ecosubtype_index_names = taxon_ecosystem_subtype_pa_df.index.tolist()\n",
    "ecosubtype_index_names = {index:str(re.sub(r'[\\s/-]', '_', index)) for index in ecosubtype_index_names}\n",
    "taxon_ecosystem_subtype_pa_df.rename(index=ecosubtype_index_names, inplace=True)\n",
    "\n",
    "# from this ecosystem subtype df, drop the Unclassified row\n",
    "taxon_ecosystem_subtype_pa_df.drop('Unclassified', inplace=True, axis=0)\n",
    "# drop all columns that have only 0s\n",
    "taxon_ecosystem_type_pa_df = taxon_ecosystem_type_pa_df.loc[(taxon_ecosystem_type_pa_df != '0').any(axis=1)]\n",
    "taxon_ecosystem_subtype_pa_df = taxon_ecosystem_subtype_pa_df.loc[(taxon_ecosystem_subtype_pa_df != '0').any(axis=1)]\n",
    "\n",
    "# write the pa matrices to files\n",
    "taxon_ecosystem_type_pa_df.to_csv(os.path.join(filtered_dir, 'pa_matrix.ecosystem_type.numerical.tsv'), sep='\\t')\n",
    "taxon_ecosystem_subtype_pa_df.to_csv(os.path.join(filtered_dir, 'pa_matrix.ecosystem_subtype.numerical.tsv'), sep='\\t')\n",
    "\n",
    "# binarize the matrices\n",
    "taxon_ecosystem_type_pa_df = taxon_ecosystem_type_pa_df.map(lambda x: '1' if x != '0' else '0')\n",
    "taxon_ecosystem_subtype_pa_df = taxon_ecosystem_subtype_pa_df.map(lambda x: '1' if x != '0' else '0')\n",
    "\n",
    "# Concatenate the rows into a single string per taxon and write out as FASTA file\n",
    "taxon_ecosystem_type_fasta_df = pd.DataFrame(taxon_ecosystem_type_pa_df.astype(str).apply(''.join, axis=0), columns=['sequence'])\n",
    "taxon_ecosystem_type_fasta_df.index = '>' + taxon_ecosystem_type_fasta_df.index.str.strip()\n",
    "taxon_ecosystem_type_fasta_df.to_csv(f\"{filtered_dir}/pa_matrix.ecosystem_type.binary.fasta\", sep='\\n', header=False)\n",
    "\n",
    "taxon_ecosystem_subtype_fasta_df = pd.DataFrame(taxon_ecosystem_subtype_pa_df.astype(str).apply(''.join, axis=0), columns=['sequence'])\n",
    "taxon_ecosystem_subtype_fasta_df.index = '>' + taxon_ecosystem_subtype_fasta_df.index.str.strip()\n",
    "taxon_ecosystem_subtype_fasta_df.to_csv(f\"{filtered_dir}/pa_matrix.ecosystem_subtype.binary.fasta\", sep='\\n', header=False)\n",
    "\n",
    "print(f\"Presence-absence matrices written out.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgt_analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
