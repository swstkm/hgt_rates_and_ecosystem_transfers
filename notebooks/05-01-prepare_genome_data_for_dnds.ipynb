{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for dN/dS analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download genome data\n",
    "\n",
    "We have gff files of the genomes of all the NOGs in the dataset already. We will download the genomes of the species in the dataset from NCBI, using a script that calls 'datasets' from the NCBI command line tools, but in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress warning from ete3 because it's not up to date with py3.12\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import gzip\n",
    "import base64\n",
    "from io import StringIO\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import AlignIO\n",
    "from Bio.Nexus import Nexus\n",
    "import tqdm\n",
    "import ete3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directories\n",
    "data_dir = os.path.abspath(\"../data/\")\n",
    "genome_data_dir = os.path.join(data_dir, \"genome_data/\")\n",
    "filtered_dir = os.path.join(data_dir, \"filtered/\")\n",
    "algs_dir = os.path.join(filtered_dir, \"single_copy_nog_algs/\")\n",
    "\n",
    "dnds_dir = os.path.join(data_dir, \"inferences\", \"dn_ds\")\n",
    "genome_tree_filepath = os.path.join(\n",
    "    data_dir, \"genome_tree\", \"genome_tree.iqtree.treefile.rooted.labeled\"\n",
    ")\n",
    "dnds_input_dir = os.path.join(dnds_dir, \"input\")\n",
    "if not os.path.exists(dnds_input_dir):\n",
    "    print(f\"Creating directory {dnds_input_dir}\")\n",
    "    os.makedirs(dnds_input_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from genome_data/ get all .gff files and extract the genome accession ID from their headers into a list\n",
    "genome_accession_ids = []\n",
    "for file in os.listdir(genome_data_dir):\n",
    "    if file.endswith(\".gff\"):\n",
    "        with open(genome_data_dir + file) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"#!genome-build-accession\"):\n",
    "                    genome_accession_ids.append(line.split(':')[1].strip())\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"No genome accession ID found in \" + file)\n",
    "\n",
    "# write the genome accession IDs to a file\n",
    "with open(os.path.join(genome_data_dir, \"genome_accession_ids.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(genome_accession_ids) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download this list of genome assembly accessions from NCBI in parallel.\n",
    "\n",
    "```bash\n",
    "python code/download_ncbi_genome_sequences.py -m acc -i data/genome_data/genome_accession_ids.txt -o data/genome_data/ -k ~/ncbi_credentials.txt 2> data/nohup_genome_fna_dload.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare NOG nucleotide sequence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the nucleotide sequences of the NOGs from the genomes of the species in the dataset.\n",
    "\n",
    "```bash\n",
    "python code/prepare_nog_nucl_seqs.py -m data/filtered/map.nog_members_single_copy.tsv  -j data/filtered/gene_features.json -g data/genome_data -o data/filtered/single_copy_nog_algs/ 2>&1 | tee data/prepare_nog_nucl_seqs.log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing AA alignments: 100%|██████████| 3718/3718 [00:04<00:00, 800.23it/s] \n"
     ]
    }
   ],
   "source": [
    "# extract the compressed alignments from the nog-> alg mapping file\n",
    "nog_to_alg_filepath = \"../data/filtered/map.nog_alg.filtered.tsv\"\n",
    "single_copy_nog_members_map_filepath = \"../data/filtered/map.nog_members_single_copy.tsv\"\n",
    "filtered_taxa_w_available_gff_filepath = os.path.join(\n",
    "    filtered_dir, 'taxa.overlap_filtered.gff_filtered.txt')\n",
    "\n",
    "if not os.path.exists(algs_dir):\n",
    "    os.makedirs(algs_dir, exist_ok=True)\n",
    "\n",
    "# read in the taxa with available GFF files\n",
    "# read it in as a set\n",
    "filtered_taxa_w_available_gff_set = set(pd.read_csv(\n",
    "    filtered_taxa_w_available_gff_filepath, header=None, names=['taxon_id'], dtype=str)['taxon_id'].tolist())\n",
    "\n",
    "# read in the single copy nog members map\n",
    "single_copy_nog_members_map = pd.read_csv(single_copy_nog_members_map_filepath, sep=\"\\t\", header=None,\n",
    "                                          names=[\"nog\", \"members\"], dtype=str)\n",
    "single_copy_nogs_set = set(single_copy_nog_members_map[\"nog\"].tolist())\n",
    "# read in the nog to alg mapping\n",
    "nog_to_alg = pd.read_csv(nog_to_alg_filepath, sep=\"\\t\", header=None,\n",
    "                         names=[\"nog\", \"algs\"], dtype=str)\n",
    "# filter out the single copy nogs\n",
    "single_copy_nog_to_alg_df = nog_to_alg[nog_to_alg[\"nog\"].isin(\n",
    "    single_copy_nogs_set)]\n",
    "\n",
    "# fn to process an alignment and keep only those with taxa that have available GFF files\n",
    "def process_alignment(args):\n",
    "    row, taxa_set = args\n",
    "    nog_name = row['nog']\n",
    "    compressed_algs = row['algs']\n",
    "    # decompress the alignment and decode it\n",
    "    alg_string = StringIO(gzip.decompress(base64.b64decode(compressed_algs)).decode())\n",
    "    # read the alignment\n",
    "    alignment = AlignIO.read(alg_string, format='fasta')\n",
    "    # retain only the taxa that are present in the pruned tree\n",
    "    pruned_alignment = [record for record in alignment if record.id.split('.')[0] in taxa_set]\n",
    "    # write each alignment in a separate file per OG\n",
    "    with open(os.path.join(algs_dir, f'{nog_name}.faa'), 'w') as f:\n",
    "        for record in pruned_alignment:\n",
    "            f.write(f'>{record.id.split(\".\")[0]}\\n{record.seq}\\n')\n",
    "\n",
    "# process the alignments in parallel\n",
    "algs_args = [(row, filtered_taxa_w_available_gff_set) for _, row in single_copy_nog_to_alg_df.iterrows()]\n",
    "with mp.Pool(mp.cpu_count() - 2) as pool:\n",
    "    list(tqdm.tqdm(pool.imap_unordered(process_alignment, algs_args), total=len(single_copy_nog_to_alg_df), desc='Writing AA alignments'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input files for Hyphy BUSTED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branches with ecotype transfers: 227, branches without ecotype transfers: 89\n"
     ]
    }
   ],
   "source": [
    "# first, read in the output of `count` to find which branches have transfers\n",
    "count_compiled_branchwise_tsv = os.path.join(data_dir, 'compiled_results', 'compiled_transfers.branchwise.ecotype.count.tsv')\n",
    "count_compiled_branchwise = pd.read_csv(count_compiled_branchwise_tsv, sep='\\t', header=0, dtype=str)\n",
    "branches_w_transfers = count_compiled_branchwise[count_compiled_branchwise['transfers'] != '0']['branch'].dropna().tolist()\n",
    "branches_wo_transfers = count_compiled_branchwise[count_compiled_branchwise['transfers'] == '0']['branch'].dropna().tolist()\n",
    "print(f'Branches with ecotype transfers: {len(branches_w_transfers)}, branches without ecotype transfers: {len(branches_wo_transfers)}')\n",
    "\n",
    "# write the lists of branches with and without transfers to files\n",
    "branches_w_transfers_filepath = os.path.join(data_dir, 'compiled_results', 'branches_w_ecotype_transfers.count.txt')\n",
    "branches_wo_transfers_filepath = os.path.join(data_dir, 'compiled_results', 'branches_wo_ecotype_transfers.count.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting protein alignments to nucleotide alignments: 100%|██████████| 3718/3718 [00:09<00:00, 401.56it/s]\n",
      "Preparing hyphy tree files and rewriting taxa IDs: 100%|██████████| 3718/3718 [00:22<00:00, 166.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty alignment files: 38 files: ['/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D3Q46.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D5CU4.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG4822.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D133R.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG3080.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG1939.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG4161.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D4X8R.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D50IS.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG3417.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG5567.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG3164.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D67P7.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG2965.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D3G5N.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D5PIX.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D4CU2.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG2302.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D4A7S.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D2UUI.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D2UJ0.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D1SN3.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG4467.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D4I18.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D5IA8.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D5GWC.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D4T41.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D6ND5.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG4043.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG0717.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D922T.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D91K7.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D17K7.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG1678.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG2825.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D9XDT.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/COG0096.aln.fna', '/root/work/projects/hgt_ecosystem/data/filtered/single_copy_nog_algs/D9IQR.aln.fna']\n",
      "Skipped 1115 trees because they had less than 2 branches with/without transfers: ['D3W6Z', 'D3VBY', 'D3NVB', 'D3VGD', 'D3S0N', 'D3P4A', 'D3QZP', 'D3RX7', 'D3MYN', 'D3VBR', 'D3P46', 'D3PQX', 'D3RI7', 'D3WC1', 'D3SGH', 'D3QZA', 'D3W3U', 'D3V98', 'D3TIV', 'D3P49', 'D3WCP', 'D3MU3', 'D3QNK', 'D3QD9', 'D3UTE', 'D3NHQ', 'D3S3Y', 'COG2316', 'D3P40', 'D3VBT', 'D3P44', 'D3NEB', 'D3UZ3', 'D3P43', 'D3P47', 'D3SSE', 'D3PGW', 'D1FZS', 'D1JTR', 'D3TI8', 'D1CZV', 'D1CGD', 'D3T7R', 'D1JQ6', 'D1C2H', 'D3UQS', 'D1G2D', 'D1JX9', 'D1DE1', 'COG1901', 'D1D19', 'D1F0D', 'D1JXX', 'D1I67', 'D1G5Y', 'D1F17', 'D1EI4', 'D1FN0', 'D1JUM', 'D1E7K', 'D1JXA', 'D1DHA', 'D1JAV', 'D1C2I', 'D1JK1', 'D1H8W', 'D1GXH', 'D1DGY', 'D1JEV', 'D5753', 'D57RZ', 'D5DPY', 'D5A7J', 'D5807', 'D59Y1', 'D5E4Z', 'D58PX', 'D5C0B', 'D5DJE', 'D59XG', 'D58X6', 'D568V', 'D56EB', 'D5C0D', 'D58XJ', 'D5A3G', 'D5E3W', 'D5B4U', 'D5C6K', 'D57MX', 'D5C9S', 'D5DE2', 'D5BDV', 'D5DBZ', 'D56JD', 'D58QM', 'D5B0R', 'D11ET', 'D0VF6', 'D0W64', 'D0Z2Y', 'D0YYJ', 'COG2108', 'D0VAI', 'D12TA', 'D0ZEH', 'D1127', 'D0V1K', 'D0ZGY', 'D12SG', 'D0YXF', 'D0YRA', 'D0XDR', 'D0W1T', 'D0YWI', 'D0XV9', 'D0XKQ', 'D0YYQ', 'D0ZBJ', 'D12F1', 'D0YXS', 'D0ZXH', 'D7X0Z', 'D7S72', 'D7YQJ', 'D7XAZ', 'D7YI6', 'D7V1D', 'D7V8S', 'D7XCD', 'D7W8F', 'D7Y5X', 'D7VS0', 'D7UEX', 'D7XKC', 'D7YDI', 'D7U4J', 'D7WC8', 'COG1768', 'D7UIY', 'D7R7A', 'D7YDJ', 'D7VJN', 'D7YQP', 'D7V94', 'D6SIW', 'D6VJK', 'D6W78', 'D6W2E', 'D6T2E', 'D6VFT', 'D7VSD', 'D6SIX', 'D6SUJ', 'D6Q5Z', 'D6VPV', 'D6R3J', 'D6QTI', 'D6R02', 'D6PYN', 'D6U2P', 'D6QJV', 'D6V08', 'D6R73', 'D6XY6', 'D6U3A', 'D6U2Q', 'D6U27', 'D6TQY', 'D6VKJ', 'D6Q8Y', 'COG4727', 'D6W7C', 'D6QK1', 'D6RH1', 'D54Z7', 'D55BP', 'D54GM', 'D4YQ8', 'D50VZ', 'D51VM', 'D6S1A', 'D4Y4S', 'D4ZMD', 'D6T1S', 'D53GJ', 'D4XZC', 'D556B', 'D51QR', 'D51VK', 'D537N', 'D52VI', 'D4ZUK', 'D4YFR', 'D55M1', 'D5157', 'D4Z28', 'D41NS', 'D3ZZI', 'D43YY', 'D43Z1', 'D4Y5A', 'D42NI', 'D40VD', 'D43G0', 'D40KH', 'D438C', 'D3Z49', 'D3Z46', 'D41P2', 'D3YDZ', 'D3WQG', 'D3WQH', 'D43IM', 'D40SA', 'D43DP', 'D43HQ', 'D3XR0', 'D43YZ', 'D3XBX', 'COG4741', 'D3YDW', 'D441Q', 'COG3432', 'D40QW', 'D3XUZ', 'D43AU', 'D43AK', 'D80WB', 'D84QX', 'D82J1', 'D841U', 'D82GX', 'D82U3', 'D86NA', 'D84I7', 'D851Z', 'D86IN', 'D85FG', 'D8405', 'D84CM', 'D81UH', 'D82GW', 'D858N', 'D86W3', 'D82HG', 'D821D', 'D82GY', 'D81EJ', 'D80WD', 'D878Q', 'D81T1', 'D82GS', 'D80WC', 'D84I6', 'D847Q', 'D84HX', 'D8420', 'D2IUZ', 'D2HQA', 'D2EZ6', 'D2DRD', 'D2EKK', 'D2HEE', 'D2I5M', 'D2F7B', 'D2IDJ', 'D2G3P', 'D2F7G', 'D2IQY', 'D2F48', 'D2F79', 'D2F78', 'D2F7F', 'D2KCJ', 'D2F7C', 'D2CRR', 'D2JAR', 'D2GJN', 'D2FYF', 'D2HKG', 'D2DKA', 'D2G3T', 'D2I28', 'D2F7A', 'D2FWW', 'D2FY7', 'D2JUJ', 'D2K4D', 'D2K4B', 'D6BZT', 'D68PD', 'D6CXE', 'D6A2F', 'D6883', 'D6DYV', 'D69AV', 'D6CI2', 'D6BQM', 'D68DU', 'D6C9G', 'D6DFR', 'D67NN', 'D6CPF', 'D6AQU', 'D6EJK', 'D6E68', 'D67QD', 'D68V0', 'D66FI', 'D6CI1', 'COG2248', 'D6BZW', 'D6BZU', 'D6EI6', 'D6B5P', 'D6AWX', 'D66FP', 'D6EIA', 'D6EX2', 'D68ZS', 'D6EPH', 'D8RY6', 'D8U1C', 'D8WGU', 'D8WWK', 'D8XG9', 'D8RP7', 'D8WW0', 'D8XC7', 'D8U1A', 'D8SU4', 'D8V79', 'D8XDN', 'D8Y3J', 'D8VAM', 'D8WQZ', 'D8S5Z', 'D8QU9', 'D8Y3N', 'D8Y2J', 'D8U9B', 'D8VW4', 'D8XA2', 'D8RQ7', 'D8UR2', 'D8W3U', 'D8S3E', 'D8TNK', 'D8SCZ', 'COG5554', 'D8XG7', 'D78H7', 'D8YMD', 'D76ZE', 'D7843', 'D7A3H', 'D774X', 'D77SJ', 'D7DM8', 'D7DDD', 'D77M6', 'D78RJ', 'D7D0G', 'D78VR', 'D7ATV', 'D786Q', 'D7A0T', 'D7CJM', 'D7BYY', 'D77IM', 'D7D0H', 'D7F1B', 'D78NF', 'D7BI7', 'D7AQH', 'D7E4V', 'D27DJ', 'D2BQW', 'D25FN', 'D28JZ', 'D263W', 'D243F', 'D27A8', 'D27F5', 'D25N2', 'D2AX7', 'D26WW', 'D27B8', 'D28VF', 'D26IW', 'D240U', 'D28E8', 'D240B', 'D29CI', 'D2CEK', 'D270K', 'D263H', 'D27WY', 'D3DWI', 'D3KCY', 'D3DPA', 'D2AJV', 'D3EPZ', 'COG2430', 'D3EV1', 'D3HE9', 'D276I', 'D3DZF', 'D3MPA', 'D3F76', 'D3F75', 'D3H6X', 'D3EBG', 'D3KR5', 'D3GWD', 'D3JZY', 'D3EV4', 'D3F8C', 'D3HDT', 'D3KF8', 'D3GWG', 'D3DST', 'D3M9D', 'D5XJ8', 'D3DZJ', 'D5U7Q', 'D3HHI', 'D5T0K', 'D5UNZ', 'D5QT6', 'D5RW0', 'D3F67', 'D5VPR', 'D5VFZ', 'D5S8P', 'D5VG1', 'D5VTH', 'D5XA1', 'D5PEH', 'D5RWR', 'D5S0U', 'D5QSQ', 'D5RUM', 'D5W84', 'D5TWJ', 'D5QZC', 'D5VG2', 'D5U4S', 'D5XJA', 'D5T58', 'D3EH9', 'D5PPI', 'D5UK1', 'D5U4C', 'D5SZ4', 'D5RUQ', 'D5PWM', 'D5ST9', 'D5P2R', 'D5W7C', 'D5QBZ', 'D5UBS', 'D5WG8', 'D5TN0', 'D49QJ', 'D48KV', 'D46EN', 'D4ATN', 'D4918', 'D48RZ', 'D4D8Q', 'D49MK', 'D4A0B', 'D45MT', 'D4AGU', 'D73QA', 'D74BC', 'D74RK', 'D75H1', 'D74RM', 'D6Z0U', 'COG1892', 'D74RN', 'D718C', 'D74PZ', 'D76HF', 'D74UI', 'D74RQ', 'D74RP', 'D7178', 'D731Y', 'D7693', 'D764R', 'D6Y8P', 'D74RJ', 'D711J', 'D75VA', 'D76IQ', 'D704P', 'D73XW', 'D731K', 'D73CK', 'D75N6', 'D8A04', 'D74Y3', 'D8FYQ', 'D73NS', 'D8DWD', 'D8EHR', 'D87JY', 'D8G39', 'D8BPC', 'D87W1', 'D8FZP', 'D8FQ6', 'D8DA6', 'D8AFT', 'D8D53', 'D8EAA', 'D8DX3', 'D8AEZ', 'D8D6K', 'D8D74', 'D8DIB', 'D8D52', 'COG5630', 'D8E9C', 'D8EYE', 'D87VR', 'D88PW', 'D8D6I', 'D8AB3', 'D8C40', 'D8C21', 'D8CY3', 'D8DDR', 'D8G15', 'D8FFM', 'D8EA7', 'D2RBE', 'D8EH6', 'D2MER', 'D2VDC', 'D2RYA', 'D2VQE', 'D2QAW', 'D8DV2', 'D2QMY', 'D2PI7', 'D2S8N', 'D2R7P', 'D2T9X', 'D2THP', 'COG1371', 'D2MMI', 'D2RVU', 'D2T0V', 'D2TPB', 'D2TV2', 'D2MSB', 'D2QMS', 'D2UN0', 'D2NSV', 'D2U2Z', 'D2PFM', 'D2MQP', 'D2MN9', 'D2UPW', 'D2SGU', 'D1TMH', 'D1SJI', 'D1KWI', 'D1RX9', 'D1RZM', 'D1U6Q', 'D1QCR', 'D1RX6', 'D1NF7', 'D1P5G', 'D1UNA', 'D1UN5', 'D1NBF', 'D1QGG', 'D1NK6', 'D1U4Q', 'D1Q3H', 'D1V7C', 'D1U1I', 'D1S7M', 'D1MXA', 'D1QYN', 'D1RY3', 'D1U4V', 'D1UNG', 'D1SN1', 'D1S9U', 'D1P8S', 'D1V2T', 'D1S72', 'D1QMY', 'D1SJN', 'D1S0U', 'D1QGF', 'D1S7E', 'D1S79', 'D2WF6', 'D2Z6C', 'D31QW', 'D2Z2K', 'D31YG', 'D30UI', 'D1P03', 'D31D5', 'D2WWY', 'D1T8F', 'D2YGZ', 'D2XYD', 'D32C0', 'D2Y31', 'D32K6', 'D2YZ7', 'D337I', 'D2VXV', 'D34D2', 'D30S8', 'D31MB', 'D2Y0F', 'D2X4Q', 'D302W', 'D2X2T', 'D2W58', 'D2XM3', 'D2ZJ0', 'D32AJ', 'D2ZY7', 'D306J', 'D34E0', 'D31FN', 'D31WS', 'D30S9', 'D30FX', 'D31AX', 'D321F', 'D3206', 'D34GC', 'D2WTS', 'D307Y', 'D2YF3', 'D2ZVV', 'D3AFI', 'D2ZC1', 'D2YG1', 'D35EB', 'D3CWI', 'D3ATS', 'D3CJK', 'D368A', 'D3AQA', 'D3C5Y', 'D364G', 'D3AUK', 'D390R', 'D35FE', 'D39ED', 'D34MH', 'D38HH', 'D3AUR', 'D39BI', 'D35Y3', 'D35VN', 'D38Z1', 'D3533', 'COG1099', 'D3CJV', 'D39G5', 'D3APP', 'D39P5', 'D381S', 'D373Y', 'D3AZE', 'D37IK', 'D37DQ', 'D3C8V', 'D35WY', 'D3539', 'D4EKG', 'D4JJR', 'D39QR', 'D4H5D', 'D4MUQ', 'D4IWR', 'D4FIX', 'D4GXI', 'D4GPD', 'D4N80', 'D4IWQ', 'D4MVF', 'D4KZ2', 'D4IUJ', 'D4JY2', 'D4NA9', 'D4JRX', 'D4IQP', 'D4IQ8', 'D4IMM', 'D4JRE', 'D4EUZ', 'D4E6N', 'D4M8C', 'D4JSS', 'D4GCY', 'D4K2D', 'D4E8W', 'D4J1E', 'D4K1M', 'D4M6Y', 'D4G07', 'D4IFY', 'COG1148', 'D4H1J', 'D4JDV', 'D8H8M', 'D8QMC', 'D8J3T', 'D8J55', 'D8H8S', 'D8J3U', 'D8IFY', 'D4JK5', 'D8K0V', 'D8M9I', 'D8HCP', 'D8JMX', 'D8HFZ', 'D8J53', 'D8H8P', 'D8H8R', 'D8JYZ', 'D8M8W', 'D8NX1', 'D8HIT', 'D8MD9', 'D8HGZ', 'D8HF4', 'D8H8N', 'D8H8K', 'D8QAT', 'D5ESY', 'D8GQ9', 'D5GV3', 'D5JUS', 'D5G6G', 'D5EDW', 'D5M29', 'D5GWS', 'D5MV0', 'D5NUD', 'D5H4R', 'D5ESW', 'D5M1R', 'D5JJR', 'D5MAI', 'D5IF7', 'D5M77', 'D5J0C', 'D5ESV', 'D5MD8', 'D5HQB', 'D5NPJ', 'D5M1X', 'D5HCP', 'D5KVD', 'D5ESZ', 'D5JUD', 'D5G0X', 'D5MQR', 'D5ESX', 'D5MC1', 'D5K8Y', 'D5ESU', 'D5I2V', 'D4PY9', 'D4QZ3', 'D4UNC', 'D5FHV', 'D5KHF', 'D4PEK', 'D4U11', 'D4R9I', 'D4SFP', 'COG4088', 'D4V6C', 'D4R9B', 'D4UCM', 'D4RB0', 'D4VUZ', 'D4PP3', 'D4UWN', 'D4SZ3', 'D4S6Z', 'D4NPC', 'COG1625', 'D4VMP', 'D4WJ8', 'D4TN3', 'D4U22', 'D4VGX', 'D4NP6', 'D4TBI', 'D4VRB', 'D4RKT', 'D4WES', 'D4NYU', 'D4R7Q', 'D6H1E', 'D6JP7', 'D6FEC', 'D6ISJ', 'D6EZB', 'D6N6G', 'D6K1W', 'D6HQV', 'D6N6H', 'D6N0P', 'D6HHJ', 'D6MUG', 'D6G7G', 'D6FPJ', 'D6N6F', 'D6F8F', 'D6IS5', 'D6N6C', 'D6KDI', 'D6PDV', 'D6KQN', 'D6J91', 'D6INH', 'D6IR7', 'D6GDV', 'D6F5V', 'D6NWQ', 'D6GV6', 'D6G62', 'D6FJD', 'D6K23', 'D6KWQ', 'D6FSR', 'D6P2H', 'D6JFK', 'D6KY2', 'D6NFK', 'D6K42', 'D6GW1', 'D7NGX', 'D7PIN', 'D7GE5', 'D7IWC', 'D7IDP', 'D7N27', 'D7PDG', 'D7MJ5', 'D7FNU', 'D7IMH', 'D7H0V', 'D7JUH', 'D7PFG', 'D7JVG', 'D7JS8', 'D7HXX', 'D7HEN', 'D7N28', 'D7M48', 'D7J8E', 'D7N9X', 'D7K9C', 'D7ICB', 'D7N26', 'D7I8E', 'D6M6M', 'D7I83', 'D7J94', 'D7JZ0', 'D7NSM', 'D7K5N', 'D7PWE', 'D7PNU', 'D7NQH', 'COG1243', 'D7Q5J', 'D7FNH', 'D7K98', 'D7PHH', 'D95SG', 'D94AA', 'D92RK', 'D95W3', 'D96SF', 'D92PA', 'D90GR', 'D91QW', 'D92HK', 'D8ZNU', 'D97G0', 'D90NF', 'D91WD', 'D9669', 'D9758', 'D8ZYG', 'D93TZ', 'COG2811', 'D952S', 'D97KN', 'D9365', 'D94GM', 'D90GQ', 'D94MN', 'D949P', 'D94MP', 'D91II', 'D9044', 'D91PZ', 'D91WE', 'COG4337', 'D95U5', 'D90A5', 'D949K', 'D19UE', 'D15HS', 'D152T', 'D14QX', 'D159I', 'D19ZN', 'D19RN', 'D1BX6', 'D1B7D', 'D1BF2', 'D1BX1', 'D1A8C', 'D18Q1', 'D1598', 'D1599', 'D17KE', 'D18JK', 'D5Z1Z', 'D5YWR', 'D5ZI9', 'D5Z1K', 'D60IE', 'D60VS', 'D63CZ', 'D647B', 'D64IY', 'D60GZ', 'D5Y8M', 'D5Z4P', 'D600P', 'D65BJ', 'D62N9', 'D616V', 'D5XTE', 'D65H5', 'D5YZP', 'D60GY', 'D660U', 'D5XXD', 'D620S', 'D61BA', 'D61TF', 'D63GY', 'D60A0', 'D5XTB', 'D654G', 'D63GT', 'D60ER', 'D61JA', 'D60GW', 'D61BE', 'D63TU', 'D65HS', 'D5Y07', 'D61P7', 'D61PQ', 'D60GX', 'D64NC', 'D9XK5', 'D9XFP', 'D9V41', 'D9TFJ', 'D9SBM', 'D9WB1', 'D9XVG', 'D9ZGY', 'D9ZR4', 'D9S2E', 'D9UNM', 'D9Z7C', 'D9WE3', 'D9ZEU', 'D9TSM', 'D9ZCT', 'D9UF0', 'D9XFK', 'D9TP8', 'DA0YY', 'DA2FK', 'DA072', 'DA2NT', 'DA46I', 'DA1KK', 'DA265', 'DA2GM', 'DA26G', 'DA283', 'DA6AQ', 'DA2HK', 'DA2BJ', 'DA5VR', 'DA5WJ', 'DA5WI', 'DA06Y', 'DA13E', 'DA685', 'DA683', 'DA3JW', 'DA2NZ', 'DA0B9', 'DA1KX', 'DA2FX', 'DA50S', 'COG4984', 'DA2XX', 'DA68R', 'DA3JT', 'D1Y1K', 'D1ZKN', 'D20MB', 'D22WE', 'D20PN', 'D1YY0', 'D1ZJM', 'D21WQ', 'D1VC5', 'D21E3', 'D208T', 'D1W8S', 'D239E', 'D22RM', 'D20D4', 'D1XBK', 'D21E9', 'D1Y4D', 'D20M3', 'D1ZV1', 'D1X0J', 'D1XTI', 'D1YVR', 'D9ET1', 'D9DS5', 'D9C6V', 'D99ID', 'D9G4I', 'D9GEY', 'D9CXW', 'COG3900', 'COG5612', 'D9GEZ', 'D9EVY', 'D9CDH', 'D9E23', 'D1X1W', 'D99XG', 'D9GGM', 'D9CZ2', 'D9FS9', 'D98H1', 'D9DNX', 'D9G0K', 'D9BX7', 'D9B5X', 'D9BX2', 'D9FXY', 'D9GHG', 'D9E7A', 'D99K6', 'D9MN8', 'D9QTY', 'COG4865', 'D9PNN', 'D9GSI', 'D9I0F', 'D9R8F', 'D9KDH', 'D9RCT', 'D9NVU', 'D9KZN', 'D9QTV', 'D9HID', 'D9MFY', 'D9HFP', 'COG4328', 'D9M0Q', 'D9K8M', 'D9JCD', 'D9I9V', 'D9MFQ', 'D9PNM', 'D9M0W', 'D9KGQ', 'D9HFM', 'D9MFU', 'D9M15', 'D9HWI', 'D9P6N', 'D9IAN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_pal2nal(args):\n",
    "    i, j = args\n",
    "    cmd = [\n",
    "        'perl', '/root/bin/pal2nal.v14/pal2nal.pl', \n",
    "        i, os.path.join(algs_dir, f'{j}.fna'),\n",
    "        '-output', 'fasta',\n",
    "        # '-nogap' # do not allow gaps in the output\n",
    "    ]\n",
    "    output_file = os.path.join(algs_dir, f'{j}.aln.fna')\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        result = subprocess.run(cmd, stdout=out_f, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error processing {j}: {result.stderr}\")\n",
    "\n",
    "# Prepare the arguments for parallel processing\n",
    "pal2nal_args = [(i, os.path.basename(i).replace('.faa', '')) for i in glob.glob(os.path.join(algs_dir, '*.faa'))]\n",
    "\n",
    "# Convert the protein alignments to nucleotide alignments using pal2nal in parallel\n",
    "with mp.Pool(mp.cpu_count() - 2) as pool:\n",
    "    list(tqdm.tqdm(pool.imap_unordered(convert_pal2nal, pal2nal_args), total=len(pal2nal_args), desc='Converting protein alignments to nucleotide alignments'))\n",
    "\n",
    "# Prepare hyphy tree files for each alignment and rewrite taxa IDs\n",
    "gtree = ete3.Tree(genome_tree_filepath, format=1)\n",
    "empty_aln_files = []\n",
    "\n",
    "aln_treefile_filepaths = []\n",
    "aln_treefile_fg_filepaths = []\n",
    "aln_treefile_bg_filepaths = []\n",
    "skip_tree_ogs = []\n",
    "branch_count_dict = {}\n",
    "for i in tqdm.tqdm(glob.glob(os.path.join(algs_dir, '*.aln.fna')), desc='Preparing hyphy tree files and rewriting taxa IDs'):\n",
    "    with open(i) as f:\n",
    "        aln_taxa_list = [line.strip().strip('>') for line in f if line.startswith('>')]\n",
    "        if not aln_taxa_list:\n",
    "            empty_aln_files.append(i)\n",
    "            continue\n",
    "    pruned_tree = gtree.copy()\n",
    "    pruned_tree.prune(aln_taxa_list)\n",
    "\n",
    "    # Rewrite taxa IDs in the alignments and treefiles\n",
    "    j = os.path.basename(i).replace('.aln.fna', '')\n",
    "    try:\n",
    "        aln = AlignIO.read(i, format='fasta')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {i}: {e}\")\n",
    "        continue\n",
    "    for record in aln:\n",
    "        if not record.id.startswith('L'):\n",
    "            record.id = f'L{record.id}'\n",
    "            record.description = ''\n",
    "    for leaf in pruned_tree:\n",
    "        if not leaf.name.startswith('L'):\n",
    "            leaf.name = 'L' + leaf.name\n",
    "    \n",
    "    tree_taxa_list = [leaf.name for leaf in pruned_tree]\n",
    "\n",
    "    for b, node_name in enumerate(branches_wo_transfers):\n",
    "        if not node_name.startswith('L') and not node_name.startswith('N'):\n",
    "            branches_wo_transfers[b] = 'L' + node_name\n",
    "    for b, node_name in enumerate(branches_w_transfers):\n",
    "        if not node_name.startswith('L') and not node_name.startswith('N'):\n",
    "            branches_w_transfers[b] = 'L' + node_name\n",
    "\n",
    "    # write out this tree to a file\n",
    "    tree_out_filepath = os.path.join(dnds_input_dir, f'{j}.treefile')\n",
    "    tree_out_filepath_fg = os.path.join(dnds_input_dir, f'{j}.fg.treefile')\n",
    "    tree_out_filepath_bg = os.path.join(dnds_input_dir, f'{j}.bg.treefile')\n",
    "    pruned_tree.write(outfile=tree_out_filepath, format=1)\n",
    "\n",
    "    fg_suffix = '{Test}'\n",
    "    bg_suffix = ''\n",
    "\n",
    "    # Create two copies of pruned_tree\n",
    "    pruned_tree_fg = pruned_tree.copy()\n",
    "    pruned_tree_bg = pruned_tree.copy()\n",
    "\n",
    "    # Note: we are making two copies of the pruned tree, \n",
    "    # one where the branches with transfers are labeled as foreground branches,\n",
    "    # and the other where the branches without transfers are labeled as foreground branches.\n",
    "    # BUSTED results should be symmetric wrt which tree is used, \n",
    "    # and we can check this by comparing the results from the two runs.\n",
    "\n",
    "    # first check if each set of branches has more than one branch in the tree\n",
    "    w_transfer_nodes = [node for node in pruned_tree_fg.traverse() if node.name in branches_w_transfers]\n",
    "    wo_transfer_nodes = [node for node in pruned_tree_fg.traverse() if node.name in branches_wo_transfers]\n",
    "    \n",
    "    if len(w_transfer_nodes) < 2 or len(wo_transfer_nodes) < 2:\n",
    "        skip_tree_ogs.append(j)\n",
    "        continue\n",
    "\n",
    "    for node in pruned_tree_fg.traverse():\n",
    "        if node.name in branches_w_transfers:\n",
    "            node.name += fg_suffix\n",
    "        elif node.name in branches_wo_transfers:\n",
    "            node.name += bg_suffix\n",
    "\n",
    "    for node in pruned_tree_bg.traverse():\n",
    "        if node.name in branches_w_transfers:\n",
    "            node.name += bg_suffix\n",
    "        elif node.name in branches_wo_transfers:\n",
    "            node.name += fg_suffix\n",
    "\n",
    "    # Write out the alignment file\n",
    "    aln_out_filepath = os.path.join(dnds_input_dir, f'{j}.aln.fna')\n",
    "    AlignIO.write(aln, aln_out_filepath, format='fasta')\n",
    "\n",
    "    # Write out the tree files\n",
    "    pruned_tree_fg.write(outfile=tree_out_filepath_fg, format=1)\n",
    "    pruned_tree_bg.write(outfile=tree_out_filepath_bg, format=1)\n",
    "\n",
    "    # Append pairs of filepaths to the list\n",
    "    aln_treefile_fg_filepaths.append((aln_out_filepath, tree_out_filepath_fg))\n",
    "    aln_treefile_bg_filepaths.append((aln_out_filepath, tree_out_filepath_bg))\n",
    "    aln_treefile_filepaths.append((aln_out_filepath, tree_out_filepath))\n",
    "\n",
    "    # in a different file, we write out for each NOG, the number of branches with and without transfers, and total\n",
    "    branch_count_dict[j] = {\n",
    "        'with_transfers': len(w_transfer_nodes),\n",
    "        'without_transfers': len(wo_transfer_nodes),\n",
    "        'total': len(w_transfer_nodes) + len(wo_transfer_nodes)\n",
    "    }\n",
    "    \n",
    "\n",
    "print(f\"Empty alignment files: {len(empty_aln_files)} files: {empty_aln_files}\")\n",
    "print(f\"Skipped {len(skip_tree_ogs)} trees because they had less than 2 branches with/without transfers: {skip_tree_ogs}\")\n",
    "# write out this list of skipped trees to a file\n",
    "skip_tree_ogs_filepath = os.path.join(dnds_dir, 'skipped_tree_ogs.txt')\n",
    "with open(skip_tree_ogs_filepath, 'w') as f:\n",
    "    f.write('\\n'.join(skip_tree_ogs) + '\\n')\n",
    "\n",
    "# write out the pair of aln and tree filepaths to a TSV file\n",
    "busted_input_filepaths_fg_tsv = os.path.join(dnds_dir, 'map.aln_treefile_input_filepaths.fg.tsv')\n",
    "with open(busted_input_filepaths_fg_tsv, 'w') as f:\n",
    "    for aln, tree in aln_treefile_fg_filepaths:\n",
    "        f.write(f\"{aln}\\t{tree}\\n\")\n",
    "\n",
    "busted_input_filepaths_bg_tsv = os.path.join(dnds_dir, 'map.aln_treefile_input_filepaths.bg.tsv')\n",
    "with open(busted_input_filepaths_bg_tsv, 'w') as f:\n",
    "    for aln, tree in aln_treefile_bg_filepaths:\n",
    "        f.write(f\"{aln}\\t{tree}\\n\")\n",
    "\n",
    "busted_input_filepaths_tsv = os.path.join(dnds_dir, 'map.aln_treefile_input_filepaths.tsv')\n",
    "with open(busted_input_filepaths_tsv, 'w') as f:\n",
    "    for aln, tree in aln_treefile_filepaths:\n",
    "        f.write(f\"{aln}\\t{tree}\\n\")\n",
    "\n",
    "# write out the branch count dict to a file\n",
    "branch_count_dict_filepath = os.path.join(dnds_dir, 'branch_count.tsv')\n",
    "# make a dataframe from the dict, with nog_id as the index\n",
    "branch_count_df = pd.DataFrame.from_dict(branch_count_dict, orient='index').reset_index().rename(columns={'index': 'nog_id'})\n",
    "branch_count_df.to_csv(branch_count_dict_filepath, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Phylo\n",
    "\n",
    "gtree = ete3.Tree(genome_tree_filepath, format=1)\n",
    "# first we label the leaves of the genome tree with 'L' to match the taxa IDs in the concatenated alignment\n",
    "for leaf in gtree:\n",
    "    leaf.name = 'L' + leaf.name\n",
    "# then we add {Test} to the branches with transfers and {Reference} to the branches without transfers\n",
    "for node in gtree.traverse():\n",
    "    if node.name in branches_w_transfers:\n",
    "        node.name += '{Test}'\n",
    "    elif node.name in branches_wo_transfers:\n",
    "        node.name += '{Reference}'\n",
    "\n",
    "# write this tree to a newick file\n",
    "output_genome_tree_filepath = os.path.join(dnds_dir, 'genome_tree.iqtree.treefile.rooted.labeled.reference_test.newick')\n",
    "gtree.write(outfile=output_genome_tree_filepath, format=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgt_analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
